{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaejams/NLP-Week2-Text-Generation/blob/main/Chatbot_LLaMa_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "In this Colab Notebook, we are going to explore Llama-2 7B, a model fine-tuned for generating text & chatting.\n",
        "\n",
        "By the end of this tutorial, you'll be able to interact with this model and use it to generate conversational responses.\n",
        "\n",
        "Whether you're curious about chatbot technology or simply want to see a machine-generated response to a particular question, this notebook will serve as a comprehensive guide.\n",
        "\n",
        "## Workflow\n",
        "1. **Installations**: We'll begin by setting up our environment with the required libraries.\n",
        "2. **Prerequisites**: Ensure we have access to the Llama-2 7B model on Hugging Face.\n",
        "3. **Loading the Model & Tokenizer**: Retrieve the model and tokenizer for our session.\n",
        "4. **Creating the Llama Pipeline**: Prepare our model for generating responses.\n",
        "5. **Interacting with Llama**: Prompt the model for answers and explore its capabilities.\n",
        "\n",
        "Let's dive in!\n",
        "\n",
        "**First, change runtime to GPU.**\n",
        "\n",
        "\n",
        "You can play with Llama-2 7B Chat here: https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat"
      ],
      "metadata": {
        "id": "bWOx7lgoYHZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations\n",
        "\n",
        "Before we proceed, we need to ensure that the essential libraries are installed:\n",
        "- `Hugging Face Transformers`: Provides us with a straightforward way to use pre-trained models.\n",
        "- `PyTorch`: Serves as the backbone for deep learning operations.\n",
        "- `Accelerate`: Optimizes PyTorch operations, especially on GPU."
      ],
      "metadata": {
        "id": "DNrZtnUyYheg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "aNTmMJIMYjiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1331aa3-4985-4a2e-9a1d-b130202196cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "\n",
        "To load our desired model, `meta-llama/Llama-2-7b-chat-hf`, we first need to authenticate ourselves on Hugging Face. This ensures we have the correct permissions to fetch the model.\n",
        "\n",
        "1. Gain access to the model on Hugging Face: [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n",
        "2. Use the Hugging Face CLI to login and verify your authentication status.\n",
        "\n"
      ],
      "metadata": {
        "id": "BO2pb-EeZA95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "_6AfgF3_arYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86fc6438-19d2-44bd-db42-dd6592fc14ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `llama-lab` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `llama-lab`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "id": "6lD_oW1uavGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdac4fce-c5a1-46b4-b34a-f1c8e68db4b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli whoami' is deprecated. Use 'hf auth whoami' instead.\u001b[0m\n",
            "happy2je\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Model & Tokenizer\n",
        "\n",
        "Here, we are preparing our session by loading both the Llama model and its associated tokenizer.\n",
        "\n",
        "The tokenizer will help in converting our text prompts into a format that the model can understand and process."
      ],
      "metadata": {
        "id": "xmJHSjx4abta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-hf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "jsBrtGpZYmcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Llama Pipeline\n",
        "\n",
        "We'll set up a pipeline for text generation.\n",
        "\n",
        "This pipeline simplifies the process of feeding prompts to our model and receiving generated text as output.\n",
        "\n",
        "*Note*: This cell takes 2-3 minutes to run"
      ],
      "metadata": {
        "id": "cKHfNL76a6qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "0AqJo1R_a9IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Responses\n",
        "\n",
        "With everything set up, let's see how Llama responds to some sample queries."
      ],
      "metadata": {
        "id": "tlbsuGikcsB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llama_response(prompt: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate a response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        prompt (str): The user's input/question for the model.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response.\n",
        "    \"\"\"\n",
        "    sequences = llama_pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=256,\n",
        "    )\n",
        "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
        "\n",
        "\n",
        "\n",
        "prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "m8RwW7Axcu9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a22baecd-8f24-4d9b-a813-9bf2751957b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
            "\n",
            "You might enjoy \"The Sopranos\", \"The Wire\", \"Mad Men\", \"Deadwood\", \"Narcos\", \"Peaky Blinders\", \"Sons of Anarchy\", \"The Shield\", \"Ozark\", \"True Detective\", \"Fargo\", \"The Americans\", \"The Handmaid's Tale\", \"Stranger Things\", and \"Better Call Saul\".\n",
            "\n",
            "All of these shows are highly rated and have received critical acclaim, so you're sure to find something that suits your tastes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Queries"
      ],
      "metadata": {
        "id": "7HSx6ctIdbW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.\\\n",
        "Based on that, what language should I learn next?\\\n",
        "Give me 5 recommendations\"\"\"\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "FoYglrLvmTN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e672d5-c4ef-44f3-bc80-dd60ce0dfd21"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.Based on that, what language should I learn next?Give me 5 recommendations that are similar to Python in terms of simplicity and versatility.\n",
            "\n",
            "Python is a great language to learn, and there are several other languages that share similarities with it in terms of simplicity and versatility. Here are five recommendations that you may find interesting:\n",
            "\n",
            "1. JavaScript: JavaScript is a popular language used for web development, and it has a syntax similar to Python's. It's also a versatile language that can be used for building desktop applications, mobile apps, and server-side programming. JavaScript is widely used in the industry, and learning it can open up many job opportunities.\n",
            "2. Ruby: Ruby is a language that's known for its simplicity and readability, making it a great language for beginners. It's also a versatile language that can be used for web development, system administration, and scripting. Ruby has a large and active community, and it's known for its concise syntax and ease of use.\n",
            "3. PHP:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to learn fast?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "hk3QokHDdgx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54fa5d5-1027-49f3-9ddb-11b66594cd8e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: How to learn fast?\n",
            "\n",
            "Learning quickly is a skill that can be developed with practice and dedication. Here are some tips on how to learn fast:\n",
            "\n",
            "1. Set clear goals: Setting specific goals helps you focus your efforts and stay motivated. Write down what you want to achieve and track your progress.\n",
            "2. Break it down: Break down complex topics into smaller, manageable chunks. This helps you understand the material better and retain it longer.\n",
            "3. Use active learning techniques: Active learning involves engaging with the material rather than just passively reading or listening. Techniques include summarizing what you've read, creating flashcards, and taking practice quizzes.\n",
            "4. Practice consistently: Consistency is key to learning quickly. Set aside a specific time each day or week to practice and review the material.\n",
            "5. Get enough sleep: Sleep plays an essential role in learning and memory consolidation. Aim for 7-9 hours of sleep each night to help your brain process and retain information.\n",
            "6. Stay organized: Keep your study materials organized, and create a schedule to help you stay on track. This will help you avoid wasting time and stay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'I love basketball. Do you have any recommendations of team sports I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "HNCpOybWdc65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1be387a-565f-467d-e63f-1f84c3dc99ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I love basketball. Do you have any recommendations of team sports I might like?\n",
            "\n",
            "I'm a 5'4\" girl and I'm not very athletic, but I still want to play something competitively.\n",
            "\n",
            "Answer:\n",
            "\n",
            "If you're looking for team sports that are similar to basketball but might be more suited to your skill level and height, here are some options to consider:\n",
            "\n",
            "1. Volleyball: Volleyball is a fun and social sport that requires similar skills to basketball, such as jumping, throwing, and coordination. It's also a lower-impact sport than basketball, which means it might be easier on your joints.\n",
            "2. Soccer: Soccer is a popular team sport that involves running, jumping, and ball handling skills. It's a great workout and can be played at a variety of levels, from recreational to competitive.\n",
            "3. Handball: Handball is a fast-paced sport that combines elements of basketball and soccer. It's played with a small ball and a goal, and it requires quick reflexes and good hand-eye coordination.\n",
            "4. Ultimate Fris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to get rich?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "kzG2YoH4dil_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b41c05-d13a-4f40-941c-839f17f63782"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: How to get rich?\n",
            "\n",
            "Getting rich is not an easy task, but it is possible with the right mindset, strategy, and consistency. Here are some proven ways to help you get rich:\n",
            "\n",
            "1. Start with a clear financial goal: Define what being \"rich\" means to you and set a specific financial goal. This will help you stay motivated and focused on your path to wealth.\n",
            "2. Live below your means: Spend less than you earn and save or invest the difference. Avoid buying things you don't need and focus on building wealth, not just spending money.\n",
            "3. Invest wisely: Invest your money in assets that have a high potential for growth, such as stocks, real estate, or a small business. Do your research and consult with a financial advisor to make informed decisions.\n",
            "4. Build multiple income streams: Diversify your income sources to reduce financial risk. This could include starting a side business, investing in rental properties, or generating passive income through dividend-paying stocks or a blog.\n",
            "5. Educate yourself: Continuously learn and improve your financial literacy. Read books, attend sem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems\n",
        "\n",
        "After 3-4 prompts, the model stops giving responses. It only outputs the user prompt.\n",
        "\n",
        "To keep talking to the model, you need to restart the notebook: `Runtime -> Restart Runtime` and run the notebook again..."
      ],
      "metadata": {
        "id": "xBfybeqxr3jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make it conversational\n",
        "Let's create an interactive chat loop, where you can converse with the Llama model.\n",
        "\n",
        "Type your questions or comments, and see how the model responds!"
      ],
      "metadata": {
        "id": "De1MFB-xgavO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    get_llama_response(user_input)"
      ],
      "metadata": {
        "id": "a4xgy3NMgcwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468ae0cf-1c44-47da-bc11-b489876a078a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: henlo\n",
            "Chatbot: henlo! I am a software engineer with a passion for creating innovative solutions. I have a strong background in computer science and have worked on various projects in the past, including web and mobile applications. I am also skilled in machine learning and have experience in developing models and algorithms for various applications. I am excited to join your team and contribute to the development of cutting-edge technology. Please let me know if there are any opportunities available for me to join your team.\n",
            "\n",
            "Sincerely,\n",
            "[Your Name]\n",
            "You: what is your favorite jellycat\n",
            "Chatbot: what is your favorite jellycat?\n",
            "Jellycat is a brand that produces plush toys, and they have a wide range of adorable designs to choose from! Here are some of my favorites:\n",
            "1. Mr Whiskers - a fluffy grey cat with bright green eyes and a mischievous grin.\n",
            "2. Tiger - a big, bold and beautiful tiger with a shimmering golden coat and a fierce roar.\n",
            "3. Monkey - a cheeky little monkey with a mischievous grin and a love for bananas.\n",
            "4. Penguin - a cute and cuddly penguin with a tuxedo-style coat and a little black bow tie.\n",
            "5. Puppy - a sweet and playful puppy with a fluffy coat and a wagging tail.\n",
            "6. Bunny - a soft and cuddly bunny with a fluffy tail and a twitching nose.\n",
            "7. Kangaroo - a hopping kangaroo with a soft and cuddly body and a little pouch for storing treats.\n",
            "8. Koala\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5OW9VH2lVb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "Thanks to the Hugging Face Library, creating a pipeline to chat with llama 2 (or any other open-source LLM) is quite easy.\n",
        "\n",
        "But if you worked a lot with much larger models such as GPT-4, you need to adjust your expectations."
      ],
      "metadata": {
        "id": "s3lxvhWKqfFd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWD3HFWlr2BE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}